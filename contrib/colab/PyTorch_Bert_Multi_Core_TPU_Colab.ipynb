{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " [PyTorch Bert-TPU] Bert on Steroids -- Colab Version",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHKEkDHRpKcN",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch/TPU Bert Fine-Tune Demo\n",
        "(Run each cell separately - don't just run all)\n",
        "\n",
        "- This notebook is part of a series of tutorials on using PyTorch on Cloud TPUs. PyTorch can use Cloud TPU cores as devices with the PyTorch/XLA package. For more on PyTorch/XLA see its Github or its documentation. We also have a \"Getting Started\" Colab notebook. Additional Colab notebooks, like this one, are available on the PyTorch/XLA Github linked above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWj3au5upQh1",
        "colab_type": "text"
      },
      "source": [
        "<h3>  &nbsp;&nbsp;Use Colab Cloud TPU&nbsp;&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a></h3>\n",
        "\n",
        "* On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "* The cell below makes sure you have access to a TPU on Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0ycw8UJpSSc",
        "colab_type": "code",
        "outputId": "8d1b3001-2b79-49e8-cf8c-ec71e8c19ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'10.100.241.34:8470'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOBxO6smpTjx",
        "colab_type": "text"
      },
      "source": [
        "### [RUNME] Install Colab TPU compatible PyTorch/TPU wheels and dependencies\n",
        "This may take up to ~2 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37F_yGAYpV8d",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "6be1097b-c68b-4176-ee0c-25d966f21aa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Installs PyTorch, PyTorch/XLA, and Torchvision\n",
        "# Copy this cell into your own notebooks to use PyTorch on Cloud TPUs \n",
        "# Warning: this may take a couple minutes to run\n",
        "\n",
        "import collections\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "VERSION = \"torch_xla==nightly\"\n",
        "CONFIG = {\n",
        "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "}[VERSION]\n",
        "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# Update TPU XRT version\n",
        "def update_server_xrt():\n",
        "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "      XRT_VERSION=CONFIG.server,\n",
        "  )\n",
        "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "update = threading.Thread(target=update_server_xrt)\n",
        "update.start()\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!pip install transformers\n",
        "!sudo apt-get install libomp5\n",
        "update.join()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updating server-side XRT to XRT-dev20200214 ...\n",
            "Uninstalling torch-1.5.0a0+ecd3c25:\n",
            "  Successfully uninstalled torch-1.5.0a0+ecd3c25\n",
            "Uninstalling torchvision-0.6.0a0+3e94dff:\n",
            "  Successfully uninstalled torchvision-0.6.0a0+3e94dff\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "-\n",
            "Operation completed over 1 objects/80.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][111.4 MiB/111.4 MiB]                                                \n",
            "Operation completed over 1 objects/111.4 MiB.                                    \n",
            "Done updating server-side XRT: <Response [200]>\n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[31mERROR: fastai 1.0.60 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.5.0a0+ecd3c25\n",
            "Processing ./torch_xla-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "  Found existing installation: torch-xla 0.8+f77a947\n",
            "    Uninstalling torch-xla-0.8+f77a947:\n",
            "      Successfully uninstalled torch-xla-0.8+f77a947\n",
            "Successfully installed torch-xla-0.8+f77a947\n",
            "Processing ./torchvision-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.5.0a0+ecd3c25)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.17.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (6.2.2)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3e94dff\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.11)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp5 is already the newest version (5.0.1-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZv23uP1pWrG",
        "colab_type": "text"
      },
      "source": [
        "# [IMP] Using Kaggle Google Quest Comp Dataset here For Demonstration purposes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK-gZNqoYcQk",
        "colab_type": "code",
        "outputId": "e037762b-2ce4-4579-dec3-ed1c44d6eea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "# This Will Download the DataSet which is hosted on my Google Drive\n",
        "!pip install gdown\n",
        "!gdown --id \"1OHOc7ltJYDRrCc2zZGKne5L9gSo-UThs\" --output \"quest.zip\"\n",
        "!unzip -q \"quest.zip\"\n",
        "#### Please Download the data from the kaggle Competiton Google - Quest if the above fails \n",
        "#### Had uploaded the datast to Google Drive here https://drive.google.com/file/d/1OHOc7ltJYDRrCc2zZGKne5L9gSo-UThs"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.8)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OHOc7ltJYDRrCc2zZGKne5L9gSo-UThs\n",
            "To: /content/quest.zip\n",
            "5.09MB [00:00, 160MB/s]\n",
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: An\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDgDdSOQpaOr",
        "colab_type": "code",
        "outputId": "10b1e285-f404-4ac4-ccd7-152dba7c27c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "# some imports we need\n",
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from contextlib import contextmanager\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from scipy import stats\n",
        "\n",
        "# xla imports\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.data_parallel as dp # http://pytorch.org/xla/index.html#running-on-multiple-xla-devices-with-multithreading\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp # http://pytorch.org/xla/index.html#running-on-multiple-xla-devices-with-multiprocessing\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "import transformers, sys, os, gc\n",
        "import numpy as np, pandas as pd, math\n",
        "import torch, random, os, multiprocessing, glob\n",
        "import torch.nn.functional as F\n",
        "import torch, time\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from scipy.stats import spearmanr\n",
        "from torch import nn\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    BertTokenizer, BertModel, BertPreTrainedModel, BertForSequenceClassification, BertConfig,\n",
        "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "from transformers import BertModel, BertConfig, BertPreTrainedModel\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqhGuZfipeOZ",
        "colab_type": "text"
      },
      "source": [
        "# Using Multiple Cloud TPU Cores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k03OS3-hP2SF",
        "colab_type": "text"
      },
      "source": [
        "Working with multiple Cloud TPU cores is different than training on a single Cloud TPU core. With a single Cloud TPU core we simply acquired the device and ran the operations using it directly. To use multiple Cloud TPU cores we must use other processes, one per Cloud TPU core. This indirection and multiplicity makes multicore training a little more complex than training on a single core, but it's necessary to maximize performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0O9lE70B72Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "from torch.optim.lr_scheduler import _LRScheduler, Optimizer\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "def run(index):\n",
        "\n",
        "    def seed_everything(seed):\n",
        "        # Sets a common random seed - both for initialization and ensuring graph is the same\n",
        "        random.seed(seed)\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
        "        # Converts tokenized input to ids, masks and segments for transformer (including bert)\n",
        "\n",
        "        def return_id(str1, str2, truncation_strategy, length):\n",
        "\n",
        "            inputs = tokenizer.encode_plus(str1, str2, add_special_tokens=True, \n",
        "                                          max_length=length, truncation_strategy=truncation_strategy\n",
        "                                          )\n",
        "            \n",
        "            input_ids =  inputs[\"input_ids\"]\n",
        "            input_masks = [1] * len(input_ids)\n",
        "            input_segments = inputs[\"token_type_ids\"]\n",
        "            \n",
        "            padding_length = length - len(input_ids)\n",
        "            padding_id = tokenizer.pad_token_id\n",
        "            \n",
        "            input_ids = input_ids + ([padding_id] * padding_length)\n",
        "            input_masks = input_masks + ([0] * padding_length)\n",
        "            input_segments = input_segments + ([0] * padding_length)\n",
        "            \n",
        "            return [input_ids, input_masks, input_segments]\n",
        "        \n",
        "        input_ids_q, input_masks_q, input_segments_q = return_id(\n",
        "            title + ' ' + question, None, 'longest_first', max_sequence_length\n",
        "        )\n",
        "        \n",
        "        input_ids_a, input_masks_a, input_segments_a = return_id(\n",
        "            answer, None, 'longest_first', max_sequence_length\n",
        "        )\n",
        "        \n",
        "        return [input_ids_q, input_masks_q, input_segments_q,\n",
        "                input_ids_a, input_masks_a, input_segments_a]\n",
        "\n",
        "    def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
        "        \n",
        "        input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
        "        input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
        "        \n",
        "        for _, instance in df[columns].iterrows():\n",
        "            \n",
        "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "\n",
        "            ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
        "            \n",
        "            input_ids_q.append(ids_q)\n",
        "            input_masks_q.append(masks_q)\n",
        "            input_segments_q.append(segments_q)\n",
        "\n",
        "            input_ids_a.append(ids_a)\n",
        "            input_masks_a.append(masks_a)\n",
        "            input_segments_a.append(segments_a)\n",
        "            \n",
        "        return [\n",
        "            np.asarray(input_ids_q, dtype=np.int32), \n",
        "            np.asarray(input_masks_q, dtype=np.int32),\n",
        "            np.asarray(input_segments_q, dtype=np.int32),\n",
        "            np.asarray(input_ids_a, dtype=np.int32), \n",
        "            np.asarray(input_masks_a, dtype=np.int32), \n",
        "            np.asarray(input_segments_a, dtype=np.int32),\n",
        "        ]\n",
        "\n",
        "    def compute_output_arrays(df, columns):\n",
        "        return np.asarray(df[columns])\n",
        "\n",
        "    def compute_spearmanr_ignore_nan(trues, preds):\n",
        "        \n",
        "        rhos = []\n",
        "        for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
        "            rhos.append(spearmanr(tcol, pcol).correlation)\n",
        "        return np.nanmean(rhos)\n",
        "\n",
        "    def train_model(train_loader, length, model, optimizer, criterion, scheduler=None):\n",
        "        \n",
        "        max_grad_norm = 1.0\n",
        "        tracker = xm.RateTracker()\n",
        "        avg_loss = 0.\n",
        "        model.train();\n",
        "        len_loader = length\n",
        "        tk0 = enumerate(train_loader)\n",
        "\n",
        "        for idx, batch in tk0:\n",
        "            \n",
        "            input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a, labels = batch\n",
        "            input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a, labels = input_ids_q.to(device, dtype=torch.long), input_masks_q.to(device, dtype = torch.long), input_segments_q.to(device, dtype = torch.long), input_ids_a.to(device, dtype = torch.long), input_masks_a.to(device, dtype = torch.long), input_segments_a.to(device, dtype = torch.long), labels.to(device, dtype = torch.float)\n",
        "            \n",
        "            logits = model(\n",
        "                q_id = input_ids_q, q_mask = input_masks_q, q_atn = input_segments_q, \n",
        "                a_id = input_ids_a, a_mask = input_masks_a, a_atn = input_segments_a\n",
        "            )\n",
        "            \n",
        "            # Computes loss\n",
        "            loss = criterion(logits, labels) \n",
        "            # BackWard pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Note: optimizer_step uses the implicit Cloud TPU context to\n",
        "            #  coordinate and synchronize gradient updates across processes.\n",
        "            #  This means that each process's network has the same weights after\n",
        "            #  this is called.\n",
        "            # Warning: this coordination requires the actions performed in each \n",
        "            #  process are the same. In more technical terms, the graph that\n",
        "            #  PyTorch/XLA generates must be the same across processes.\n",
        "            xm.optimizer_step(optimizer) # Note: barrier=True not needed when using ParallelLoader \n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            avg_loss += loss.item() / len_loader\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "              print('[xla:{}] ({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
        "                xm.get_ordinal(), idx, loss.item(), tracker.rate(),\n",
        "                tracker.global_rate(), time.asctime()), flush=True\n",
        "              )\n",
        "\n",
        "            del input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a, labels\n",
        "        gc.collect()\n",
        "        return avg_loss\n",
        "\n",
        "    def val_model(val_loader, model, length, criterion, val_shape, batch_size=4):\n",
        "\n",
        "        avg_val_loss = 0\n",
        "        model.eval();\n",
        "        len_loader = length\n",
        "\n",
        "        valid_preds = []\n",
        "        original    = []\n",
        "        \n",
        "        for idx, batch in enumerate(val_loader):\n",
        "            \n",
        "            input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a, labels = batch\n",
        "            input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a, labels = input_ids_q.to(device, dtype=torch.long), input_masks_q.to(device, dtype = torch.long), input_segments_q.to(device, dtype = torch.long), input_ids_a.to(device, dtype = torch.long), input_masks_a.to(device, dtype = torch.long), input_segments_a.to(device, dtype = torch.long), labels.to(device, dtype = torch.float)\n",
        "\n",
        "            logits = model(\n",
        "                q_id = input_ids_q, q_mask = input_masks_q, q_atn = input_segments_q, \n",
        "                a_id = input_ids_a, a_mask = input_masks_a, a_atn = input_segments_a\n",
        "            )\n",
        "\n",
        "            avg_val_loss += criterion(logits, labels).item() / len_loader\n",
        "            valid_preds.append(logits.detach().cpu().squeeze().numpy())\n",
        "            original.append(labels.detach().cpu().squeeze().numpy())\n",
        "\n",
        "        return avg_val_loss, np.vstack(valid_preds), np.vstack(original)\n",
        "\n",
        "    class QuestDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, inputs, labels=None):\n",
        "            \n",
        "            self.inputs = inputs\n",
        "            self.labels = labels\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            \n",
        "            input_ids_q       = self.inputs[0][idx]\n",
        "            input_masks_q     = self.inputs[1][idx]\n",
        "            input_segments_q  = self.inputs[2][idx]\n",
        "            \n",
        "            input_ids_a       = self.inputs[3][idx]\n",
        "            input_masks_a     = self.inputs[4][idx]\n",
        "            input_segments_a  = self.inputs[5][idx]\n",
        "            \n",
        "            if self.labels is not None:\n",
        "                labels = self.labels[idx]\n",
        "                return input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a, labels\n",
        "            return input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.inputs[0])\n",
        "\n",
        "    class BertForSequenceClassification_TF_Port(BertPreTrainedModel):\n",
        "        \n",
        "        def __init__(self, config):\n",
        "\n",
        "            super(BertForSequenceClassification_TF_Port, self).__init__(config)\n",
        "            \n",
        "            self.config     = config\n",
        "            self.activation = nn.Tanh() # ----added Tanh\n",
        "            self.num_labels = config.num_labels\n",
        "            self.bert       = BertModel(config)\n",
        "            self.dropout    = nn.Dropout(config.hidden_dropout_prob)\n",
        "            self.classifier = nn.Linear(config.hidden_size*2, 30)\n",
        "            self.init_weights()\n",
        "\n",
        "        def forward(self, q_id, a_id, q_mask, a_mask, q_atn, a_atn):\n",
        "            \n",
        "            q_embedding = self.bert(q_id, attention_mask = q_mask, token_type_ids = q_atn)\n",
        "            a_embedding = self.bert(a_id, attention_mask = a_mask, token_type_ids = a_atn)\n",
        "            q = torch.mean(q_embedding[0], 1)\n",
        "            a = torch.mean(a_embedding[0], 1)\n",
        "            logits = self.classifier(self.dropout(self.activation(torch.cat([q, a], 1))))\n",
        "            return logits\n",
        "\n",
        "    SEED = 42\n",
        "    MAX_SEQUENCE_LENGTH = 384\n",
        "    seed_everything(SEED)\n",
        "    \n",
        "    PATH = './'\n",
        "    BERT_PATH = './'\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    dfx     = pd.read_csv(\"train.csv\", nrows = 1024).fillna('none')\n",
        "    sample  = pd.read_csv(\"sample_submission.csv\")\n",
        "    df_test = pd.read_csv(\"test.csv\").fillna(\"\")\n",
        "    \n",
        "    target_cols = list(sample.drop(\"qa_id\", axis = 1).columns)\n",
        "    df_train, df_valid = train_test_split(dfx, random_state=42, test_size=0.2)\n",
        "    \n",
        "    df_train.reset_index(drop=True, inplace=True)\n",
        "    df_valid.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    output_categories = list(df_train.columns[11:])\n",
        "    input_categories  = list(df_train.columns[[1,2,5]])\n",
        "    \n",
        "    train_targets = df_train[target_cols].values\n",
        "    valid_targets = df_valid[target_cols].values\n",
        "\n",
        "    bert_model_config      = 'bert_config.json'\n",
        "    bert_config            = BertConfig.from_pretrained('bert-base-uncased')\n",
        "    bert_config.num_labels = 30\n",
        "    bert_config.output_hidden_states = True\n",
        "\n",
        "    BATCH_SIZE = 4 # OOM\n",
        "    epochs = 7\n",
        "    ACCUM_STEPS = 1\n",
        "\n",
        "    scores      = []\n",
        "    valid_preds = []\n",
        "    test_preds  = []\n",
        "\n",
        "    xm.master_print(\"Preparing train datasets....\")\n",
        "    train_outputs     = compute_output_arrays(df_train, output_categories)\n",
        "    train_outputs     = torch.tensor(train_outputs, dtype=torch.float32)\n",
        "    train_inputs      = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "    \n",
        "    xm.master_print(\"Preparing Valid datasets....\")\n",
        "    valid_outputs     = compute_output_arrays(df_valid, output_categories)\n",
        "    valid_outputs     = torch.tensor(valid_outputs, dtype=torch.float32)\n",
        "    valid_inputs      = compute_input_arrays(df_valid, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    xm.master_print(\"Preparing Dataloaders....\")\n",
        "    \n",
        "    # Note: each process has its own (identical) copies of each dataset.\n",
        "    train_set     = QuestDataset(inputs=train_inputs, labels=train_outputs)\n",
        "\n",
        "    # Creates the (distributed) train sampler, which let this process only access \n",
        "    # its portion of the training dataset.\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            train_set,\n",
        "            num_replicas=xm.xrt_world_size(),\n",
        "            rank=xm.get_ordinal(),\n",
        "            shuffle=True,\n",
        "            )\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, \n",
        "                                               batch_size=BATCH_SIZE, \n",
        "                                               sampler = train_sampler,\n",
        "                                               drop_last=True,\n",
        "                                               )\n",
        "    train_dl_len = len(train_loader)\n",
        "\n",
        "    valid_set = QuestDataset(inputs=valid_inputs, labels=valid_outputs)\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            valid_set,\n",
        "            num_replicas = xm.xrt_world_size(),\n",
        "            rank=xm.get_ordinal(),\n",
        "            shuffle=False,\n",
        "            )\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_set, \n",
        "                                               batch_size = BATCH_SIZE, \n",
        "                                               drop_last = False,\n",
        "                                               sampler = valid_sampler,\n",
        "                                               )\n",
        "        \n",
        "    val_dl_len = len(valid_loader)\n",
        "\n",
        "    best_score       = -1.\n",
        "    best_param_score = None\n",
        "    best_param_epoch = None\n",
        "        \n",
        "    learning_rate = 4e-5 * xm.xrt_world_size() # Scale learning rate to num cores\n",
        "    # Get loss function, device, optimizer and model\n",
        "    # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
        "    \n",
        "    device = xm.xla_device()\n",
        "        \n",
        "    model = BertForSequenceClassification_TF_Port.from_pretrained('bert-base-uncased', config=bert_config).to(device)\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    \n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.00}\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=4e-5)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    xm.master_print(\"Training....\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        xm.master_print(f\"Epoch.... {epoch}\")\n",
        "\n",
        "        start_time          = time.time()\n",
        "        train_para_loader   = pl.ParallelLoader(train_loader, [device])\n",
        "        avg_loss            = train_model(train_para_loader.per_device_loader(device), \n",
        "                                          train_dl_len, model, optimizer, criterion, \n",
        "                                          scheduler=None\n",
        "                                          )\n",
        "\n",
        "        valid_para_loader = pl.ParallelLoader(valid_loader, [device])\n",
        "        avg_val_loss, preds, original = val_model(valid_para_loader.per_device_loader(device), \n",
        "                                                  model, val_dl_len, criterion, \n",
        "                                                  val_shape=valid_outputs.shape[0],  \n",
        "                                                  batch_size=BATCH_SIZE\n",
        "                                                  )\n",
        "        spear = []\n",
        "\n",
        "        for jj in range(preds.shape[1]):\n",
        "          p1, p2 = list(original[:, jj]), list(torch.sigmoid(torch.from_numpy(np.asarray(list(preds[:, jj])))).detach().cpu().numpy())\n",
        "          coef, _ = np.nan_to_num(stats.spearmanr(p1, p2))\n",
        "          spear.append(coef)\n",
        "        score = np.mean(spear)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        if xm.is_master_ordinal():\n",
        "          xm.master_print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(\n",
        "            epoch + 1, epochs, avg_loss, avg_val_loss, score, elapsed_time)\n",
        "        )\n",
        "\n",
        "        if best_score < score:\n",
        "            best_score = score\n",
        "            best_param_score = model.state_dict()\n",
        "            best_epoch_score = epoch\n",
        "        \n",
        "        xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "\n",
        "    xm.master_print(f'Best model came from Epoch {best_epoch_score+1} with score of {best_score}',)\n",
        "    model.load_state_dict(best_param_score)\n",
        "    scores.append(best_score)\n",
        "    \n",
        "    xm.master_print('Individual Fold Scores:')\n",
        "    xm.master_print(scores)\n",
        "    xm.master_print(f'QUEST BERT-base CV score: {np.mean(scores)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWHybzVFRFBz",
        "colab_type": "text"
      },
      "source": [
        "```spawn()``` takes a function (the \"map function\"), a tuple of arguments (the placeholder flags dict), the number of processes to create, and whether to create these new processes by \"forking\" or \"spawning.\" While spawning new processes is generally recommended, Colab only supports forking.\n",
        "\n",
        "```spawn()``` will create eight processes, one for each Cloud TPU core, and call ```run()``` -- the map function -- on each process. The inputs to ```run()``` are an ```index``` (zero through seven) and the ```placeholder flags```. When the proccesses acquire their device they actually acquire their corresponding Cloud TPU core automatically.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK0Ty-68LJWO",
        "colab_type": "code",
        "outputId": "a260c6a0-d1a0-432b-8ef0-4508213eea7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") # to disable those warnings from TPU's and The Metric Calculations \n",
        "\n",
        "xmp.spawn(run, nprocs=8, start_method='fork') # 1 also works (with slightl better score) # NB I haven't finetuned the parameters as such"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing train datasets....\n",
            "Preparing Valid datasets....\n",
            "Preparing Dataloaders....\n",
            "Training....\n",
            "Epoch.... 0\n",
            "[xla:0] (0) Loss=0.70650 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:44:57 2020\n",
            "[xla:5] (0) Loss=0.70750 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:44:57 2020\n",
            "[xla:4] (0) Loss=0.71442 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:44:57 2020\n",
            "[xla:2] (0) Loss=0.71106 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:44:57 2020\n",
            "[xla:3] (0) Loss=0.70956 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:44:57 2020\n",
            "[xla:6] (0) Loss=0.71739 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:44:57 2020\n",
            "[xla:1] (0) Loss=0.70790 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:44:57 2020\n",
            "[xla:7] (0) Loss=0.71384 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:44:57 2020\n",
            "Epoch 1/7 \t loss=0.4847 \t val_loss=0.4016 \t score=0.180821 \t time=123.40s\n",
            "Finished training epoch 0\n",
            "Epoch.... 1\n",
            "[xla:0] (0) Loss=0.45904 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:44 2020\n",
            "[xla:3] (0) Loss=0.44036 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:44 2020\n",
            "[xla:7] (0) Loss=0.37445 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:44 2020\n",
            "[xla:4] (0) Loss=0.40656 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:44 2020\n",
            "[xla:5] (0) Loss=0.45986 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:44 2020\n",
            "[xla:6] (0) Loss=0.39483 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:44 2020\n",
            "[xla:2] (0) Loss=0.38961 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:44 2020\n",
            "[xla:1] (0) Loss=0.34844 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:44 2020\n",
            "Epoch 2/7 \t loss=0.4193 \t val_loss=0.3944 \t score=0.239663 \t time=14.67s\n",
            "Finished training epoch 1\n",
            "Epoch.... 2\n",
            "[xla:5] (0) Loss=0.44481 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:59 2020\n",
            "[xla:1] (0) Loss=0.33969 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:59 2020\n",
            "[xla:6] (0) Loss=0.39127 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:59 2020\n",
            "[xla:4] (0) Loss=0.40669 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:59 2020\n",
            "[xla:0] (0) Loss=0.44951 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:59 2020\n",
            "[xla:3] (0) Loss=0.43704 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:59 2020\n",
            "[xla:2] (0) Loss=0.39213 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:59 2020\n",
            "[xla:7] (0) Loss=0.35224 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:46:59 2020\n",
            "Epoch 3/7 \t loss=0.4108 \t val_loss=0.3884 \t score=0.259900 \t time=14.77s\n",
            "Finished training epoch 2\n",
            "Epoch.... 3\n",
            "[xla:2] (0) Loss=0.38516 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:13 2020\n",
            "[xla:3] (0) Loss=0.42286 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:13 2020\n",
            "[xla:1] (0) Loss=0.33729 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:13 2020\n",
            "[xla:6] (0) Loss=0.38755 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:13 2020\n",
            "[xla:7] (0) Loss=0.35212 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:13 2020\n",
            "[xla:5] (0) Loss=0.44646 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:13 2020\n",
            "[xla:0] (0) Loss=0.43609 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:13 2020\n",
            "[xla:4] (0) Loss=0.40089 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:13 2020\n",
            "Epoch 4/7 \t loss=0.4008 \t val_loss=0.3876 \t score=0.267685 \t time=14.66s\n",
            "Finished training epoch 3\n",
            "Epoch.... 4\n",
            "[xla:1] (0) Loss=0.33371 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:28 2020\n",
            "[xla:0] (0) Loss=0.41551 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:28 2020\n",
            "[xla:3] (0) Loss=0.39634 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:28 2020\n",
            "[xla:4] (0) Loss=0.38916 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:28 2020\n",
            "[xla:5] (0) Loss=0.44896 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:28 2020\n",
            "[xla:6] (0) Loss=0.37605 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:28 2020\n",
            "[xla:7] (0) Loss=0.35002 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:28 2020\n",
            "[xla:2] (0) Loss=0.37790 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:28 2020\n",
            "Epoch 5/7 \t loss=0.3903 \t val_loss=0.3843 \t score=0.275572 \t time=14.45s\n",
            "Finished training epoch 4\n",
            "Epoch.... 5\n",
            "[xla:2] (0) Loss=0.36966 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:43 2020\n",
            "[xla:4] (0) Loss=0.38059 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:43 2020\n",
            "[xla:5] (0) Loss=0.44128 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:43 2020\n",
            "[xla:6] (0) Loss=0.35792 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:43 2020\n",
            "[xla:7] (0) Loss=0.34937 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:43 2020\n",
            "[xla:0] (0) Loss=0.38197 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:43 2020\n",
            "[xla:1] (0) Loss=0.32093 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:43 2020\n",
            "[xla:3] (0) Loss=0.37541 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:43 2020\n",
            "Epoch 6/7 \t loss=0.3805 \t val_loss=0.3863 \t score=0.304296 \t time=14.69s\n",
            "Finished training epoch 5\n",
            "Epoch.... 6\n",
            "[xla:2] (0) Loss=0.37358 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:57 2020\n",
            "[xla:5] (0) Loss=0.45316 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:57 2020\n",
            "[xla:7] (0) Loss=0.36063 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:57 2020\n",
            "[xla:6] (0) Loss=0.34432 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:57 2020\n",
            "[xla:4] (0) Loss=0.36821 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:57 2020\n",
            "[xla:0] (0) Loss=0.36008 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:57 2020\n",
            "[xla:1] (0) Loss=0.31190 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:57 2020\n",
            "[xla:3] (0) Loss=0.38328 Rate=0.00 GlobalRate=0.00 Time=Sat Feb 15 05:47:57 2020\n",
            "Epoch 7/7 \t loss=0.3767 \t val_loss=0.3843 \t score=0.294337 \t time=14.66s\n",
            "Finished training epoch 6\n",
            "Best model came from Epoch 6 with score of 0.3042959513162428\n",
            "Individual Fold Scores:\n",
            "[0.3042959513162428]\n",
            "QUEST BERT-base CV score: 0.3042959513162428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZN47EJzUVJ9",
        "colab_type": "text"
      },
      "source": [
        "Additional notebooks demonstrating how to run PyTorch on Cloud TPUs can be found in PyTorch XLA contrib folder. While Colab provides a free Cloud TPU, training is even faster on Google Cloud Platform, especially when using multiple Cloud TPUs in a Cloud TPU pod. Scaling from a single Cloud TPU, like in this notebook, to many Cloud TPUs in a pod is easy, too. You use the same code as this notebook and just spawn more processes."
      ]
    }
  ]
}