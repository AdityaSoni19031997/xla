{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " [PyTorch Bert-TPU] Bert on Steroids -- Colab Version",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "786ac92147714ef9817c3164215e6b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1e49ec22062d443780a55dea3d96cf5f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_803c7cb7cf5c4d668337244d7ffbd985",
              "IPY_MODEL_64d93f70aa9a4f07862341e70ff435f8"
            ]
          }
        },
        "1e49ec22062d443780a55dea3d96cf5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "803c7cb7cf5c4d668337244d7ffbd985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_247630287dc1456dbb58f441d7074ad8",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_934bc876352e4eb6881eb36edd955ca2"
          }
        },
        "64d93f70aa9a4f07862341e70ff435f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5b4598ecd43c4829aa470fca362b00e5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 361/361 [00:00&lt;00:00, 12.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0b1b7df48ef4cd0832291209924d3c8"
          }
        },
        "247630287dc1456dbb58f441d7074ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "934bc876352e4eb6881eb36edd955ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b4598ecd43c4829aa470fca362b00e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0b1b7df48ef4cd0832291209924d3c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c177454f89d04bd09812fd2d6fb58e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce62c3ac5cc646bd8951ab0bd3b3bc0f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ced4248928cd4522ae2ce8e6e5950813",
              "IPY_MODEL_bbd60af4e0d04c97b6010f336d5e285b"
            ]
          }
        },
        "ce62c3ac5cc646bd8951ab0bd3b3bc0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ced4248928cd4522ae2ce8e6e5950813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3fb6981c74cb4213bc90ce85ae647c74",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ddb50b6779748a4b0a96bd6785a4441"
          }
        },
        "bbd60af4e0d04c97b6010f336d5e285b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4671ed3c024a4c588cff3df35f6be641",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 440M/440M [00:06&lt;00:00, 68.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_89c0383c54994dc08216eced9e6488ee"
          }
        },
        "3fb6981c74cb4213bc90ce85ae647c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ddb50b6779748a4b0a96bd6785a4441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4671ed3c024a4c588cff3df35f6be641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "89c0383c54994dc08216eced9e6488ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHKEkDHRpKcN",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch/TPU Bert Fine-Tune Demo\n",
        "(Run each cell separately - don't just run all)\n",
        "\n",
        "- This notebook is part of a series of tutorials on using PyTorch on Cloud TPUs. PyTorch can use Cloud TPU cores as devices with the PyTorch/XLA package. For more on PyTorch/XLA see its Github or its documentation. We also have a \"Getting Started\" Colab notebook. Additional Colab notebooks, like this one, are available on the PyTorch/XLA Github [link](https://github.com/pytorch/xla/tree/master/contrib/colab)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWj3au5upQh1",
        "colab_type": "text"
      },
      "source": [
        "<h3>  &nbsp;&nbsp;Use Colab Cloud TPU&nbsp;&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a></h3>\n",
        "\n",
        "* On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "* The cell below makes sure you have access to a TPU on Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0ycw8UJpSSc",
        "colab_type": "code",
        "outputId": "d57fdd9e-99ff-432b-99f1-f919a50e0b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'10.120.28.66:8470'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOBxO6smpTjx",
        "colab_type": "text"
      },
      "source": [
        "### [RUNME] Install Colab TPU compatible PyTorch/TPU wheels and dependencies\n",
        "This may take up to ~2 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37F_yGAYpV8d",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "46df37be-eb6c-42b7-f0f0-ffeeaa98f443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Installs PyTorch, PyTorch/XLA, and Torchvision\n",
        "# Copy this cell into your own notebooks to use PyTorch on Cloud TPUs \n",
        "# Warning: this may take a couple minutes to run\n",
        "\n",
        "import collections\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "VERSION = \"torch_xla==nightly\" #@param [\"xrt==1.15.0\", \"torch_xla==nightly\"] \n",
        "CONFIG = {\n",
        "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "}[VERSION]\n",
        "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# Update TPU XRT version\n",
        "def update_server_xrt():\n",
        "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "      XRT_VERSION=CONFIG.server,\n",
        "  )\n",
        "  print(url)\n",
        "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "update = threading.Thread(target=update_server_xrt)\n",
        "update.start()\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!pip install transformers\n",
        "!sudo apt-get install libomp5\n",
        "update.join()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updating server-side XRT to XRT-dev20200216 ...\n",
            "http://10.120.28.66:8475/requestversion/XRT-dev20200216\n",
            "Done updating server-side XRT: <Response [200]>\n",
            "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\n",
            "Uninstalling torchvision-0.5.0:\n",
            "  Successfully uninstalled torchvision-0.5.0\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][ 79.9 MiB/ 79.9 MiB]                                                \n",
            "Operation completed over 1 objects/79.9 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][111.6 MiB/111.6 MiB]                                                \n",
            "Operation completed over 1 objects/111.6 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[31mERROR: fastai 1.0.60 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.5.0a0+87dc2db\n",
            "Processing ./torch_xla-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-0.8+b06662c\n",
            "Processing ./torchvision-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.5.0a0+87dc2db)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.17.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (6.2.2)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+6c2cda6\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.1MB 55.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=ff35570f85b3bab2a775094aa5df88202e49747f054033aa53a99a1157f7b57b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (381 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 134443 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZv23uP1pWrG",
        "colab_type": "text"
      },
      "source": [
        "# [IMP] Using Kaggle Google Quest Comp Dataset here For Demonstration purposes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK-gZNqoYcQk",
        "colab_type": "code",
        "outputId": "4785ef8d-ba50-4d24-d1a3-a6a7511f4144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# This Will Download the DataSet which is hosted on my Google Drive\n",
        "import os;\n",
        "os.system(\"!pip install gdown\")\n",
        "!gdown --id \"1OHOc7ltJYDRrCc2zZGKne5L9gSo-UThs\" --output \"quest.zip\"\n",
        "!unzip -q \"quest.zip\"\n",
        "\n",
        "#### Please Download the data from the kaggle Competiton Google - Quest if the above fails \n",
        "#### Had uploaded the datast to Google Drive here https://drive.google.com/file/d/1OHOc7ltJYDRrCc2zZGKne5L9gSo-UThs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OHOc7ltJYDRrCc2zZGKne5L9gSo-UThs\n",
            "To: /content/quest.zip\n",
            "5.09MB [00:01, 4.36MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDgDdSOQpaOr",
        "colab_type": "code",
        "outputId": "38ec336a-0966-4842-f298-5885cf2ffd24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "# Some imports we need\n",
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "\n",
        "# xla imports\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.data_parallel as dp # To read more, http://pytorch.org/xla/index.html#running-on-multiple-xla-devices-with-multithreading\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp # To read more, http://pytorch.org/xla/index.html#running-on-multiple-xla-devices-with-multiprocessing\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "import sys, os, gc\n",
        "import random, multiprocessing, glob\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "from transformers import (\n",
        "    BertTokenizer, BertModel, BertPreTrainedModel, \n",
        "    BertForSequenceClassification, BertConfig, AdamW,\n",
        ")\n",
        "from contextlib import contextmanager\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqhGuZfipeOZ",
        "colab_type": "text"
      },
      "source": [
        "# Using Multiple Cloud TPU Cores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k03OS3-hP2SF",
        "colab_type": "text"
      },
      "source": [
        "Working with multiple Cloud TPU cores is different than training on a single Cloud TPU core. With a single Cloud TPU core we simply acquired the device and ran the operations using it directly. To use multiple Cloud TPU cores we must use other processes, one per Cloud TPU core. This indirection and multiplicity makes multicore training a little more complex than training on a single core, but it's necessary to maximize performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RENPYz6QfZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
        "    # Converts tokenized input to ids, masks and segments for transformer (including bert)\n",
        "\n",
        "    def return_id(str1, str2, truncation_strategy, length):\n",
        "        \n",
        "        # adds the special [CLS], [SEP] tokens etc automatically..\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            str1, str2, add_special_tokens=True, \n",
        "            max_length=length, truncation_strategy=truncation_strategy\n",
        "        )\n",
        "        \n",
        "        input_ids =  inputs[\"input_ids\"]\n",
        "        input_masks = [1] * len(input_ids) # attention masks\n",
        "        input_segments = inputs[\"token_type_ids\"]\n",
        "        \n",
        "        padding_length = length - len(input_ids)\n",
        "        padding_id = tokenizer.pad_token_id\n",
        "        \n",
        "        input_ids = input_ids + ([padding_id] * padding_length)\n",
        "        input_masks = input_masks + ([0] * padding_length)\n",
        "        input_segments = input_segments + ([0] * padding_length)\n",
        "        \n",
        "        return [input_ids, input_masks, input_segments]\n",
        "    \n",
        "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
        "        title + ' ' + question, None, 'longest_first', max_sequence_length\n",
        "    )\n",
        "    \n",
        "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
        "        answer, None, 'longest_first', max_sequence_length\n",
        "    )\n",
        "    \n",
        "    return [\n",
        "            input_ids_q, input_masks_q, input_segments_q,\n",
        "            input_ids_a, input_masks_a, input_segments_a,\n",
        "          ]\n",
        "\n",
        "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
        "    \n",
        "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
        "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
        "    \n",
        "    for _, instance in df[columns].iterrows():\n",
        "        \n",
        "        # grab the question_title, question_body and teh answer_text from the DF\n",
        "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "\n",
        "        # converting them to the format the transformer expects...\n",
        "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
        "        \n",
        "        input_ids_q.append(ids_q)\n",
        "        input_masks_q.append(masks_q)\n",
        "        input_segments_q.append(segments_q)\n",
        "\n",
        "        input_ids_a.append(ids_a)\n",
        "        input_masks_a.append(masks_a)\n",
        "        input_segments_a.append(segments_a)\n",
        "        \n",
        "    return [\n",
        "        np.asarray(input_ids_q, dtype=np.int32), \n",
        "        np.asarray(input_masks_q, dtype=np.int32),\n",
        "        np.asarray(input_segments_q, dtype=np.int32),\n",
        "        np.asarray(input_ids_a, dtype=np.int32), \n",
        "        np.asarray(input_masks_a, dtype=np.int32), \n",
        "        np.asarray(input_segments_a, dtype=np.int32),\n",
        "    ]\n",
        "\n",
        "def compute_output_arrays(df, columns):\n",
        "    # grab the target cvalues values\n",
        "    return np.asarray(df[columns])\n",
        "\n",
        "class QuestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, inputs, labels=None):      \n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        input_ids_q       = self.inputs[0][idx]\n",
        "        input_masks_q     = self.inputs[1][idx]\n",
        "        input_segments_q  = self.inputs[2][idx]\n",
        "        \n",
        "        input_ids_a       = self.inputs[3][idx]\n",
        "        input_masks_a     = self.inputs[4][idx]\n",
        "        input_segments_a  = self.inputs[5][idx]\n",
        "        \n",
        "        if self.labels is not None:\n",
        "            labels = self.labels[idx]\n",
        "            return input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a, labels\n",
        "        return input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs[0])\n",
        "\n",
        "class CustomBert(BertPreTrainedModel):\n",
        "    \n",
        "    def __init__(self, config):\n",
        "\n",
        "        super(CustomBert, self).__init__(config)\n",
        "        \n",
        "        self.config     = config\n",
        "        self.activation = nn.Tanh()\n",
        "        self.num_labels = config.num_labels\n",
        "        self.bert       = BertModel(config)\n",
        "        self.dropout    = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size*2, 30)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, q_id, a_id, q_mask, a_mask, q_atn, a_atn):\n",
        "        \n",
        "        q_embedding = self.bert(q_id, attention_mask = q_mask, token_type_ids = q_atn)\n",
        "        a_embedding = self.bert(a_id, attention_mask = a_mask, token_type_ids = a_atn)\n",
        "        \n",
        "        q = torch.mean(q_embedding[0], 1)\n",
        "        a = torch.mean(a_embedding[0], 1)\n",
        "        \n",
        "        logits = self.classifier(self.dropout(self.activation(torch.cat([q, a], 1))))\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0O9lE70B72Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(index):\n",
        "\n",
        "    def seed_everything(seed):\n",
        "        # Sets a common random seed - both for initialization and ensuring graph is the same\n",
        "        random.seed(seed)\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed) # this should be done before firing Python itself\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "  \n",
        "    def train_model(train_loader, length, model, optimizer, criterion, scheduler=None):\n",
        "        \n",
        "        max_grad_norm = 1.0\n",
        "        tracker = xm.RateTracker()\n",
        "        avg_loss = 0.\n",
        "        model.train();\n",
        "        len_loader = length\n",
        "        tk0 = enumerate(train_loader)\n",
        "\n",
        "        for idx, batch in tk0:\n",
        "            \n",
        "            # Tensors coming off the ParallelLoader are already on TPU device. (thanks to @dlibenzi)\n",
        "            # Please refer to hugging face docs for below def's\n",
        "            # input_ids_*       -> basically this is input_ids\n",
        "            # input_masks_*     -> basically this is attention_mask\n",
        "            # input_segements_* -> basically this is token_type_ids\n",
        "\n",
        "            input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a, labels = batch\n",
        "            \n",
        "            # the below line is kinda necessary otherwise we will get dtype exceptions since XLA dtyp's are diffrent from torch dtype's\n",
        "            # otherwise it isn't necessary because Tensors coming off the ParallelLoader are already on TPU device.\n",
        "            input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a, labels = input_ids_q.to(device, dtype=torch.long), input_masks_q.to(device, dtype = torch.long), input_segments_q.to(device, dtype = torch.long), input_ids_a.to(device, dtype = torch.long), input_masks_a.to(device, dtype = torch.long), input_segments_a.to(device, dtype = torch.long), labels.to(device, dtype = torch.float)\n",
        "            \n",
        "            logits = model(\n",
        "                q_id = input_ids_q, q_mask = input_masks_q, q_atn = input_segments_q, \n",
        "                a_id = input_ids_a, a_mask = input_masks_a, a_atn = input_segments_a\n",
        "            )\n",
        "            \n",
        "            # Computes The loss\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Make The Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Note: optimizer_step uses the implicit Cloud TPU context to\n",
        "            #  coordinate and synchronize gradient updates across processes.\n",
        "            #  This means that each process's network has the same weights after\n",
        "            #  this is called.\n",
        "\n",
        "            # Warning: this coordination requires the actions performed in each \n",
        "            #  process are the same. In more technical terms, the graph that\n",
        "            #  PyTorch/XLA generates must be the same across processes.\n",
        "\n",
        "            xm.optimizer_step(optimizer) # Note: barrier=True not needed when using ParallelLoader \n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            # avg_loss += loss.item() / len_loader # This slows the overall process down hence we should avoid it..\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "              print('[xla:{}] ({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
        "                xm.get_ordinal(), idx, loss.item(), tracker.rate(),\n",
        "                tracker.global_rate(), time.asctime()), flush=True\n",
        "              )\n",
        "            del input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a, labels\n",
        "        gc.collect()\n",
        "        return # avg_loss\n",
        "\n",
        "    def val_model(val_loader, model, length, criterion, val_shape, batch_size=4):\n",
        "\n",
        "        avg_val_loss = 0\n",
        "        model.eval();\n",
        "        len_loader = length\n",
        "\n",
        "        valid_preds = []\n",
        "        original    = []\n",
        "        \n",
        "        for idx, batch in enumerate(val_loader):\n",
        "            \n",
        "            # Please refer to hugging face docs for below def's\n",
        "            # input_ids_*       -> basically this is input_ids\n",
        "            # input_masks_*     -> basically this is attention_mask\n",
        "            # input_segements_* -> basically this is token_type_ids\n",
        "            input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a, labels = batch\n",
        "\n",
        "            # the below line is kinda necessary otherwise we will get dtype exceptions since XLA dtyp's are diffrent from torch dtype's\n",
        "            # otherwise it isn't necessary because Tensors coming off the ParallelLoader are already on TPU device.\n",
        "            input_ids_q, input_masks_q, input_segments_q, input_ids_a, input_masks_a, input_segments_a, labels = input_ids_q.to(device, dtype=torch.long), input_masks_q.to(device, dtype = torch.long), input_segments_q.to(device, dtype = torch.long), input_ids_a.to(device, dtype = torch.long), input_masks_a.to(device, dtype = torch.long), input_segments_a.to(device, dtype = torch.long), labels.to(device, dtype = torch.float)         \n",
        "\n",
        "            # compute logits\n",
        "            logits = model(\n",
        "                q_id = input_ids_q, q_mask = input_masks_q, q_atn = input_segments_q, \n",
        "                a_id = input_ids_a, a_mask = input_masks_a, a_atn = input_segments_a\n",
        "            )\n",
        "\n",
        "            # avg_val_loss += criterion(logits, labels).item() / len_loader\n",
        "            \n",
        "            # grab the logits our model predicts\n",
        "            valid_preds.append(logits.detach().cpu().squeeze().numpy())\n",
        "            original.append(labels.detach().cpu().squeeze().numpy())\n",
        "\n",
        "        return np.vstack(valid_preds), np.vstack(original)\n",
        "\n",
        "    SEED = 42                 # set the SEED for reproducibility\n",
        "    MAX_SEQUENCE_LENGTH = 384 # can be as high as 512 - [Extra Tokens]\n",
        "    seed_everything(SEED)\n",
        "\n",
        "    # grab the pre-trained tokenizer from hugging face\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    dfx     = pd.read_csv(\"train.csv\", nrows = 1024).fillna('none')\n",
        "    sample  = pd.read_csv(\"sample_submission.csv\")\n",
        "    df_test = pd.read_csv(\"test.csv\").fillna(\"\")\n",
        "    \n",
        "    target_cols = list(sample.drop(\"qa_id\", axis = 1).columns)\n",
        "\n",
        "    # split the data\n",
        "    df_train, df_valid = train_test_split(dfx, random_state=42, test_size=0.2)\n",
        "    \n",
        "    df_train.reset_index(drop=True, inplace=True) # reset the index else torch datasets will have issues\n",
        "    df_valid.reset_index(drop=True, inplace=True) # reset the index else torch datasets will have issues\n",
        "\n",
        "    # grab the input and the target column names\n",
        "    output_categories = list(df_train.columns[11:])\n",
        "    input_categories  = list(df_train.columns[[1,2,5]])\n",
        "    \n",
        "    # grab train and valid target cols values\n",
        "    train_targets = df_train[target_cols].values\n",
        "    valid_targets = df_valid[target_cols].values\n",
        "\n",
        "    # our bert-model config\n",
        "    bert_model_config      = 'bert_config.json'\n",
        "    bert_config            = BertConfig.from_pretrained('bert-base-uncased')\n",
        "    bert_config.num_labels = 30\n",
        "    bert_config.output_hidden_states = True\n",
        "\n",
        "    # few hyper-params\n",
        "\n",
        "    BATCH_SIZE = 4 # OOM\n",
        "    epochs = 3\n",
        "\n",
        "    scores, valid_preds , test_preds  = [], [], []\n",
        "\n",
        "    xm.master_print(\"Preparing train datasets....\")\n",
        "\n",
        "    train_outputs     = compute_output_arrays(df_train, output_categories)\n",
        "    train_outputs     = torch.tensor(train_outputs, dtype=torch.float32)\n",
        "    train_inputs      = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "    \n",
        "    xm.master_print(\"Preparing Valid datasets....\")\n",
        "    \n",
        "    valid_outputs     = compute_output_arrays(df_valid, output_categories)\n",
        "    valid_outputs     = torch.tensor(valid_outputs, dtype=torch.float32)\n",
        "    valid_inputs      = compute_input_arrays(df_valid, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    xm.master_print(\"Preparing Dataloaders....\")\n",
        "    \n",
        "    # Note: each process has its own (identical) copies of each dataset.\n",
        "    train_set     = QuestDataset(inputs=train_inputs, labels=train_outputs)\n",
        "\n",
        "    # Creates the (distributed) train sampler, which let this process only access \n",
        "    # its portion of the training dataset.\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            train_set,\n",
        "            num_replicas=xm.xrt_world_size(),\n",
        "            rank=xm.get_ordinal(),\n",
        "            shuffle=True,\n",
        "            )\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, \n",
        "                                               batch_size=BATCH_SIZE, \n",
        "                                               sampler = train_sampler,\n",
        "                                               drop_last=True,\n",
        "                                               )\n",
        "    train_dl_len = len(train_loader)\n",
        "\n",
        "    # Note: each process has its own (identical) copies of each dataset.\n",
        "    valid_set = QuestDataset(inputs=valid_inputs, labels=valid_outputs)\n",
        "    \n",
        "    # Creates the (distributed) valid sampler, which let this process only access \n",
        "    # its portion of the validation dataset.\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            valid_set,\n",
        "            num_replicas = xm.xrt_world_size(),\n",
        "            rank=xm.get_ordinal(),\n",
        "            shuffle=False,\n",
        "            )\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_set, \n",
        "                                               batch_size = BATCH_SIZE, \n",
        "                                               drop_last = False,\n",
        "                                               sampler = valid_sampler,\n",
        "                                               )\n",
        "        \n",
        "    val_dl_len = len(valid_loader)\n",
        "\n",
        "    best_score       = -1.\n",
        "    best_param_score = None\n",
        "    best_param_epoch = None\n",
        "        \n",
        "    learning_rate = 4e-5 * xm.xrt_world_size() # Scale learning rate to num cores\n",
        "    \n",
        "    # Get loss function, device, optimizer and model\n",
        "    # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
        "    device = xm.xla_device()    \n",
        "\n",
        "    # Create the model\n",
        "    model = CustomBert.from_pretrained('bert-base-uncased', config=bert_config).to(device)\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    \n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.00}\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=4e-5)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        xm.master_print(f\"Training Epoch.... {epoch}\")\n",
        "\n",
        "        start_time          = time.time()\n",
        "        \n",
        "        # Wraps an existing PyTorch DataLoader\n",
        "        train_para_loader   = pl.ParallelLoader(train_loader, [device])\n",
        "        \n",
        "        # train the model each epoch here\n",
        "        train_model(train_para_loader.per_device_loader(device), \n",
        "                    train_dl_len, model, optimizer, criterion, \n",
        "                    scheduler=None\n",
        "                  )\n",
        "\n",
        "        # Wraps an existing PyTorch DataLoader\n",
        "        valid_para_loader = pl.ParallelLoader(valid_loader, [device])\n",
        "        \n",
        "        # validate the model here on unseen data\n",
        "        preds, original = val_model(valid_para_loader.per_device_loader(device), \n",
        "                                    model, val_dl_len, criterion, val_shape=valid_outputs.shape[0],  \n",
        "                                    batch_size=BATCH_SIZE\n",
        "                                  )\n",
        "        \n",
        "        # This is specific to competition\n",
        "        spear = []\n",
        "        \n",
        "        for jj in range(preds.shape[1]):\n",
        "          \n",
        "          # remember we get the logits from the model, so we need to do the sigmoid and put the tensor back to cpu\n",
        "          # to compute the spearman co-efficient\n",
        "          \n",
        "          p1, p2 = list(original[:, jj]), list(torch.sigmoid(torch.from_numpy(np.asarray(list(preds[:, jj])))).detach().cpu().numpy())\n",
        "          coef, _ = np.nan_to_num(stats.spearmanr(p1, p2))\n",
        "          spear.append(coef)\n",
        "        \n",
        "        score = np.mean(spear)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        if xm.is_master_ordinal():\n",
        "          # since we are using multiple-cores, we don't want each core to print things..\n",
        "          xm.master_print('Epoch {}/{} \\t score={:.6f} \\t time={:.2f}s'.format(\n",
        "            epoch + 1, epochs, score, elapsed_time)\n",
        "        )\n",
        "\n",
        "        if best_score < score:\n",
        "            best_score = score\n",
        "            best_param_score = model.state_dict()\n",
        "            best_epoch_score = epoch\n",
        "        \n",
        "        xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "\n",
        "    xm.master_print(f'Best model came from Epoch {best_epoch_score+1} with score of {best_score}',)\n",
        "    model.load_state_dict(best_param_score)\n",
        "    scores.append(best_score)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWHybzVFRFBz",
        "colab_type": "text"
      },
      "source": [
        "```spawn()``` takes a function (the \"map function\"), a tuple of arguments (the placeholder flags dict), the number of processes to create, and whether to create these new processes by \"forking\" or \"spawning.\" While spawning new processes is generally recommended, Colab only supports forking.\n",
        "\n",
        "```spawn()``` will create eight processes, one for each Cloud TPU core, and call ```run()``` -- the map function -- on each process. The inputs to ```run()``` are an ```index``` (zero through seven) and the ```placeholder flags``` (not in my case, but in general). When the proccesses acquire their device they actually acquire their corresponding Cloud TPU core automatically.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK0Ty-68LJWO",
        "colab_type": "code",
        "outputId": "abf9b7b9-6442-46b1-eb34-9a16529bb8ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880,
          "referenced_widgets": [
            "786ac92147714ef9817c3164215e6b1d",
            "1e49ec22062d443780a55dea3d96cf5f",
            "803c7cb7cf5c4d668337244d7ffbd985",
            "64d93f70aa9a4f07862341e70ff435f8",
            "247630287dc1456dbb58f441d7074ad8",
            "934bc876352e4eb6881eb36edd955ca2",
            "5b4598ecd43c4829aa470fca362b00e5",
            "f0b1b7df48ef4cd0832291209924d3c8",
            "c177454f89d04bd09812fd2d6fb58e71",
            "ce62c3ac5cc646bd8951ab0bd3b3bc0f",
            "ced4248928cd4522ae2ce8e6e5950813",
            "bbd60af4e0d04c97b6010f336d5e285b",
            "3fb6981c74cb4213bc90ce85ae647c74",
            "2ddb50b6779748a4b0a96bd6785a4441",
            "4671ed3c024a4c588cff3df35f6be641",
            "89c0383c54994dc08216eced9e6488ee"
          ]
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") # to disable those warnings from TPU's and the metric calculations \n",
        "# RAM i got When i ran this on Colab was ~35GB\n",
        "xmp.spawn(run, nprocs=8, start_method='fork') # 1 also works (with slightly better score) # NB I haven't finetuned the parameters as such"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "786ac92147714ef9817c3164215e6b1d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing train datasets....\n",
            "Preparing Valid datasets....\n",
            "Preparing Dataloaders....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c177454f89d04bd09812fd2d6fb58e71",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Epoch.... 0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[xla:0] (0) Loss=0.70650 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:14:01 2020\n",
            "[xla:5] (0) Loss=0.70750 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:14:01 2020\n",
            "[xla:1] (0) Loss=0.70790 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:14:01 2020\n",
            "[xla:7] (0) Loss=0.71384 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:14:01 2020\n",
            "[xla:3] (0) Loss=0.70956 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:14:01 2020\n",
            "[xla:2] (0) Loss=0.71106 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:14:01 2020\n",
            "[xla:6] (0) Loss=0.71739 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:14:01 2020\n",
            "[xla:4] (0) Loss=0.71442 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:14:01 2020\n",
            "Epoch 1/3 \t score=0.180821 \t time=148.63s\n",
            "Finished training epoch 0\n",
            "Training Epoch.... 1\n",
            "[xla:4] (0) Loss=0.40656 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:31 2020\n",
            "[xla:3] (0) Loss=0.44036 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:31 2020\n",
            "[xla:0] (0) Loss=0.45904 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:31 2020\n",
            "[xla:2] (0) Loss=0.38961 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:31 2020\n",
            "[xla:5] (0) Loss=0.45986 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:31 2020\n",
            "[xla:1] (0) Loss=0.34844 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:31 2020\n",
            "[xla:7] (0) Loss=0.37445 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:31 2020\n",
            "[xla:6] (0) Loss=0.39483 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:31 2020\n",
            "Epoch 2/3 \t score=0.239663 \t time=12.38s\n",
            "Finished training epoch 1\n",
            "Training Epoch.... 2\n",
            "[xla:7] (0) Loss=0.35224 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:43 2020\n",
            "[xla:0] (0) Loss=0.44951 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:43 2020\n",
            "[xla:5] (0) Loss=0.44481 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:43 2020\n",
            "[xla:6] (0) Loss=0.39127 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:43 2020\n",
            "[xla:4] (0) Loss=0.40669 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:43 2020\n",
            "[xla:2] (0) Loss=0.39213 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:43 2020\n",
            "[xla:3] (0) Loss=0.43704 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:43 2020\n",
            "[xla:1] (0) Loss=0.33969 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:15:43 2020\n",
            "Epoch 3/3 \t score=0.259900 \t time=12.51s\n",
            "Finished training epoch 2\n",
            "Best model came from Epoch 3 with score of 0.2598997509561588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvZmtwDL2pXh",
        "colab_type": "code",
        "outputId": "41ebcc28-e40d-4a10-9c36-07f1c851f8be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "%tb # some weird exception happens at the very end\n",
        "xmp.spawn(run, nprocs=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No traceback available to show.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Preparing train datasets....\n",
            "Preparing Valid datasets....\n",
            "Preparing Dataloaders....\n",
            "Training Epoch.... 0\n",
            "[xla:0] (0) Loss=0.71949 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:16:28 2020\n",
            "[xla:0] (100) Loss=0.44261 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:17:37 2020\n",
            "[xla:0] (200) Loss=0.44977 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:18:12 2020\n",
            "Epoch 1/3 \t score=0.248145 \t time=153.23s\n",
            "Finished training epoch 0\n",
            "Training Epoch.... 1\n",
            "[xla:0] (0) Loss=0.38855 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:18:53 2020\n",
            "[xla:0] (100) Loss=0.42187 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:19:28 2020\n",
            "[xla:0] (200) Loss=0.43857 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:20:04 2020\n",
            "Epoch 2/3 \t score=0.308305 \t time=80.84s\n",
            "Finished training epoch 1\n",
            "Training Epoch.... 2\n",
            "[xla:0] (0) Loss=0.35714 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:20:14 2020\n",
            "[xla:0] (100) Loss=0.39692 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:20:49 2020\n",
            "[xla:0] (200) Loss=0.42985 Rate=0.00 GlobalRate=0.00 Time=Mon Feb 17 16:21:24 2020\n",
            "Epoch 3/3 \t score=0.336307 \t time=80.47s\n",
            "Finished training epoch 2\n",
            "Best model came from Epoch 3 with score of 0.3363066069603487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZN47EJzUVJ9",
        "colab_type": "text"
      },
      "source": [
        "Additional notebooks demonstrating how to run PyTorch on Cloud TPUs can be found in PyTorch XLA [Contrib](https://github.com/pytorch/xla/tree/master/contrib/colab) folder. While Colab provides a free Cloud TPU, training is even faster on Google Cloud Platform, especially when using multiple Cloud TPUs in a Cloud TPU pod. Scaling from a single Cloud TPU, like in this notebook, to many Cloud TPUs in a pod is easy, too. You use the same code as this notebook and just spawn more processes."
      ]
    }
  ]
}